{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a64b4b57",
   "metadata": {},
   "source": [
    "## 5. H2O AutoML and Hyperparameter Search\n",
    "\n",
    "We run AutoML after establishing baselines. AutoML explores multiple model families and hyperparameters to find strong candidates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47b8f9b",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "We load core libraries and enable the local H2O environment.\n",
    "\n",
    "Logs will be written to ./h2o_logs and H2O will run in verbose mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c1bec0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T08:07:24.349572Z",
     "iopub.status.busy": "2026-02-01T08:07:24.349572Z",
     "iopub.status.idle": "2026-02-01T08:07:25.113947Z",
     "shell.execute_reply": "2026-02-01T08:07:25.113947Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure h2o is available from /tmp/pydeps\n",
    "sys.path.insert(0, '/tmp/pydeps')\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef7039b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T08:07:25.115949Z",
     "iopub.status.busy": "2026-02-01T08:07:25.115949Z",
     "iopub.status.idle": "2026-02-01T08:07:25.119983Z",
     "shell.execute_reply": "2026-02-01T08:07:25.119983Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Run configuration ---\n",
    "TEST_MODE = False  # Set to True for quick validation\n",
    "RUN_AUTOML = 1\n",
    "RUN_XGBOOST = 1\n",
    "\n",
    "AUTOML_MAX_MODELS = 20\n",
    "AUTOML_MAX_RUNTIME_SECS = 18000  # 5 hours\n",
    "AUTOML_NFOLDS = 5\n",
    "\n",
    "XGB_N_TRIALS = 15  # target 20-30 in final run\n",
    "XGB_EARLY_STOPPING_ROUNDS = 50\n",
    "XGB_DEVICE = 'cuda:0'\n",
    "XGB_TREE_METHOD = 'hist'\n",
    "\n",
    "if TEST_MODE:\n",
    "    AUTOML_MAX_MODELS = 1\n",
    "    AUTOML_MAX_RUNTIME_SECS = 300\n",
    "    AUTOML_NFOLDS = 2\n",
    "    XGB_N_TRIALS = 1\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('h2o_logs', exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3871455",
   "metadata": {},
   "source": [
    "## 2. Data loading\n",
    "\n",
    "We load the OULAD CSV files and build the same final dataset used in the main notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9787a0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T08:07:25.122304Z",
     "iopub.status.busy": "2026-02-01T08:07:25.122304Z",
     "iopub.status.idle": "2026-02-01T08:07:25.126522Z",
     "shell.execute_reply": "2026-02-01T08:07:25.126522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DATA_DIR: d:\\Project\\DBM_FINAL\\data\n"
     ]
    }
   ],
   "source": [
    "# CHANGE THIS to your local folder containing the 7 OULAD CSV files\n",
    "DEFAULT_DATA_DIR = os.path.join('.', 'data')\n",
    "\n",
    "required_files = [\n",
    "    'assessments.csv','courses.csv','studentAssessment.csv','studentInfo.csv',\n",
    "    'studentRegistration.csv','studentVle.csv','vle.csv'\n",
    "]\n",
    "\n",
    "# Resolve DATA_DIR from common locations\n",
    "candidate_dirs = [DEFAULT_DATA_DIR, '.']\n",
    "resolved = None\n",
    "for d in candidate_dirs:\n",
    "    if all(os.path.exists(os.path.join(d, f)) for f in required_files):\n",
    "        resolved = d\n",
    "        break\n",
    "\n",
    "if resolved is None:\n",
    "    missing = [f for f in required_files if not os.path.exists(os.path.join(DEFAULT_DATA_DIR, f))]\n",
    "    print('Missing files in DEFAULT_DATA_DIR:', missing)\n",
    "    print('DEFAULT_DATA_DIR currently set to:', os.path.abspath(DEFAULT_DATA_DIR))\n",
    "    print('Also checked:', os.path.abspath('.'))\n",
    "else:\n",
    "    DATA_DIR = resolved\n",
    "    print('Using DATA_DIR:', os.path.abspath(DATA_DIR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "465d0c96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T08:07:25.128527Z",
     "iopub.status.busy": "2026-02-01T08:07:25.127527Z",
     "iopub.status.idle": "2026-02-01T08:07:28.457629Z",
     "shell.execute_reply": "2026-02-01T08:07:28.457629Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_csv(name, usecols=None, dtype=None):\n",
    "    path = os.path.join(DATA_DIR, name)\n",
    "    return pd.read_csv(path, usecols=usecols, dtype=dtype, low_memory=False)\n",
    "\n",
    "assessments = read_csv('assessments.csv')\n",
    "courses = read_csv('courses.csv')\n",
    "student_info = read_csv('studentInfo.csv')\n",
    "student_reg = read_csv('studentRegistration.csv')\n",
    "student_assess = read_csv('studentAssessment.csv')\n",
    "vle = read_csv('vle.csv')\n",
    "\n",
    "student_vle = read_csv(\n",
    "    'studentVle.csv',\n",
    "    usecols=['code_module','code_presentation','id_student','id_site','date','sum_click'],\n",
    "    dtype={\n",
    "        'code_module':'category','code_presentation':'category',\n",
    "        'id_student':'int32','id_site':'int32',\n",
    "        'date':'int16','sum_click':'int32'\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bba78ad",
   "metadata": {},
   "source": [
    "## 3. Cleaning and feature engineering\n",
    "\n",
    "We apply the same cleaning and feature engineering used in the main notebook, then drop leakage features and remove zero-activity rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8bcdd22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T08:07:28.459631Z",
     "iopub.status.busy": "2026-02-01T08:07:28.459631Z",
     "iopub.status.idle": "2026-02-01T08:07:30.178105Z",
     "shell.execute_reply": "2026-02-01T08:07:30.177090Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- assessments: drop known invalid IDs\n",
    "invalid_ids = {40087, 40088}\n",
    "assessments = assessments[~assessments['id_assessment'].isin(invalid_ids)].copy()\n",
    "assessments['weight'] = pd.to_numeric(assessments['weight'], errors='coerce')\n",
    "assessments['date'] = pd.to_numeric(assessments['date'], errors='coerce')\n",
    "\n",
    "# --- studentAssessment: clean score/date\n",
    "student_assess['score'] = student_assess['score'].replace(['?', '', ' '], np.nan)\n",
    "student_assess['score'] = pd.to_numeric(student_assess['score'], errors='coerce')\n",
    "student_assess['date_submitted'] = pd.to_numeric(student_assess['date_submitted'], errors='coerce')\n",
    "student_assess['is_banked'] = pd.to_numeric(student_assess['is_banked'], errors='coerce').fillna(0).astype(int)\n",
    "student_assess = student_assess.dropna(subset=['score']).copy()\n",
    "\n",
    "# --- studentInfo cleanup\n",
    "student_info['imd_band'] = student_info['imd_band'].replace('?', np.nan)\n",
    "student_info['age_band'] = student_info['age_band'].replace({'55<=': '>=55'})\n",
    "\n",
    "# --- studentVle: drop duplicates\n",
    "student_vle = student_vle.drop_duplicates().copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c72f7637",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T08:07:30.179105Z",
     "iopub.status.busy": "2026-02-01T08:07:30.179105Z",
     "iopub.status.idle": "2026-02-01T08:07:34.243381Z",
     "shell.execute_reply": "2026-02-01T08:07:34.243381Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Than Minh\\AppData\\Local\\Temp\\ipykernel_18108\\1098740652.py:50: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  vle_type_pivot = vle_type_agg.pivot_table(\n"
     ]
    }
   ],
   "source": [
    "# --- target\n",
    "student_info['final_result_upd'] = student_info['final_result'].replace({\n",
    "    'Distinction': 'Pass',\n",
    "    'Withdrawn': 'Fail'\n",
    "})\n",
    "student_info = student_info[student_info['final_result_upd'].isin(['Pass','Fail'])].copy()\n",
    "student_info['final_result_upd_numeric'] = (student_info['final_result_upd'] == 'Pass').astype(int)\n",
    "\n",
    "# --- VLE aggregates\n",
    "vle_agg = (\n",
    "    student_vle\n",
    "    .groupby(['id_student','code_module','code_presentation'], as_index=False, observed=True)\n",
    "    .agg(\n",
    "        total_sum_click=('sum_click','sum'),\n",
    "        active_days=('date','nunique'),\n",
    "        distinct_sites=('id_site','nunique')\n",
    "    )\n",
    ")\n",
    "\n",
    "early = student_vle[student_vle['date'] <= 14]\n",
    "early_agg = (\n",
    "    early.groupby(['id_student','code_module','code_presentation'], as_index=False, observed=True)\n",
    "         .agg(early_sum_click=('sum_click','sum'), early_active_days=('date','nunique'))\n",
    ")\n",
    "\n",
    "vle_features = vle_agg.merge(early_agg, on=['id_student','code_module','code_presentation'], how='left')\n",
    "vle_features[['early_sum_click','early_active_days']] = vle_features[['early_sum_click','early_active_days']].fillna(0)\n",
    "\n",
    "vle_features['early_click_ratio'] = vle_features['early_sum_click'] / vle_features['total_sum_click'].replace(0, np.nan)\n",
    "vle_features['early_click_ratio'] = vle_features['early_click_ratio'].fillna(0)\n",
    "\n",
    "vle_features['clicks_per_active_day'] = vle_features['total_sum_click'] / vle_features['active_days'].replace(0, np.nan)\n",
    "vle_features['clicks_per_active_day'] = vle_features['clicks_per_active_day'].fillna(0)\n",
    "\n",
    "vle_features['early_clicks_per_active_day'] = vle_features['early_sum_click'] / vle_features['early_active_days'].replace(0, np.nan)\n",
    "vle_features['early_clicks_per_active_day'] = vle_features['early_clicks_per_active_day'].fillna(0)\n",
    "\n",
    "vle_features['early_active_ratio'] = vle_features['early_active_days'] / vle_features['active_days'].replace(0, np.nan)\n",
    "vle_features['early_active_ratio'] = vle_features['early_active_ratio'].fillna(0)\n",
    "\n",
    "# activity type aggregates\n",
    "vle_types = student_vle.merge(vle[['id_site','activity_type']], on='id_site', how='left')\n",
    "\n",
    "vle_type_agg = (\n",
    "    vle_types.groupby(['id_student','code_module','code_presentation','activity_type'], observed=True)\n",
    "             .agg(type_clicks=('sum_click','sum'))\n",
    "             .reset_index()\n",
    ")\n",
    "\n",
    "vle_type_pivot = vle_type_agg.pivot_table(\n",
    "    index=['id_student','code_module','code_presentation'],\n",
    "    columns='activity_type',\n",
    "    values='type_clicks',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "for col in vle_type_pivot.columns:\n",
    "    if col not in ['id_student','code_module','code_presentation']:\n",
    "        vle_type_pivot = vle_type_pivot.rename(columns={col: f'clicks_type_{col}'})\n",
    "\n",
    "vle_features = vle_features.merge(vle_type_pivot, on=['id_student','code_module','code_presentation'], how='left')\n",
    "\n",
    "# diversity features\n",
    "activity_cols = [c for c in vle_features.columns if c.startswith('clicks_type_')]\n",
    "type_sum = vle_features[activity_cols].sum(axis=1).replace(0, np.nan)\n",
    "type_probs = vle_features[activity_cols].div(type_sum, axis=0).fillna(0)\n",
    "vle_features['distinct_activity_types'] = (vle_features[activity_cols] > 0).sum(axis=1)\n",
    "vle_features['activity_entropy'] = -(type_probs * np.log(type_probs + 1e-9)).sum(axis=1)\n",
    "vle_features['top_type_share'] = type_probs.max(axis=1)\n",
    "\n",
    "vle_features['log1p_total_sum_click'] = np.log1p(vle_features['total_sum_click'])\n",
    "vle_features['log1p_early_sum_click'] = np.log1p(vle_features['early_sum_click'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcc6fae7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T08:07:34.245793Z",
     "iopub.status.busy": "2026-02-01T08:07:34.245793Z",
     "iopub.status.idle": "2026-02-01T08:07:46.435069Z",
     "shell.execute_reply": "2026-02-01T08:07:46.434064Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- assessment features (kept for completeness, later dropped)\n",
    "sa = student_assess.merge(\n",
    "    assessments[['id_assessment','code_module','code_presentation','assessment_type','weight','date']],\n",
    "    on='id_assessment', how='left'\n",
    ")\n",
    "\n",
    "sa['weighted_score'] = np.where(sa['is_banked'] == 1, 0, sa['score'] * (sa['weight'] / 100.0))\n",
    "sa['lateness_days'] = sa['date_submitted'] - sa['date']\n",
    "\n",
    "assess_agg = (\n",
    "    sa.groupby(['id_student','code_module','code_presentation'], as_index=False, observed=True)\n",
    "      .agg(\n",
    "          exam_weighted=('weighted_score', lambda x: x[sa.loc[x.index,'assessment_type'].eq('Exam')].sum()),\n",
    "          non_exam_weighted=('weighted_score', lambda x: x[~sa.loc[x.index,'assessment_type'].eq('Exam')].sum()),\n",
    "          mean_score=('score','mean'),\n",
    "          late_submissions=('lateness_days', lambda s: (s>0).sum())\n",
    "      )\n",
    ")\n",
    "\n",
    "assess_agg['has_exam'] = (assess_agg['exam_weighted'] > 0).astype(int)\n",
    "assess_agg['has_non_exam'] = (assess_agg['non_exam_weighted'] > 0).astype(int)\n",
    "assess_agg['overall_grade'] = np.where(\n",
    "    (assess_agg['has_exam']==1) & (assess_agg['has_non_exam']==1),\n",
    "    (assess_agg['exam_weighted'] + assess_agg['non_exam_weighted']) / 2.0,\n",
    "    np.where(assess_agg['has_exam']==1, assess_agg['exam_weighted'], assess_agg['non_exam_weighted'])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20aadec5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T08:07:46.436071Z",
     "iopub.status.busy": "2026-02-01T08:07:46.436071Z",
     "iopub.status.idle": "2026-02-01T08:07:46.460089Z",
     "shell.execute_reply": "2026-02-01T08:07:46.460089Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- registration features\n",
    "student_reg['date_registration'] = pd.to_numeric(student_reg['date_registration'], errors='coerce')\n",
    "student_reg['date_unregistration'] = pd.to_numeric(student_reg['date_unregistration'], errors='coerce')\n",
    "\n",
    "reg_features = student_reg[['id_student','code_module','code_presentation','date_registration','date_unregistration']].copy()\n",
    "reg_features['unregistered_flag'] = reg_features['date_unregistration'].notna().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "470c1f1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T08:07:46.462413Z",
     "iopub.status.busy": "2026-02-01T08:07:46.462413Z",
     "iopub.status.idle": "2026-02-01T08:07:46.581977Z",
     "shell.execute_reply": "2026-02-01T08:07:46.581977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped rows with NA in feature list: 6827\n",
      "Dropped leakage features: ['overall_grade', 'exam_weighted', 'non_exam_weighted', 'mean_score', 'late_submissions', 'final_result', 'final_result_upd']\n"
     ]
    }
   ],
   "source": [
    "# --- build final dataset\n",
    "final_data = (\n",
    "    student_info\n",
    "      .merge(vle_features, on=['id_student','code_module','code_presentation'], how='left')\n",
    "      .merge(assess_agg, on=['id_student','code_module','code_presentation'], how='left')\n",
    "      .merge(reg_features, on=['id_student','code_module','code_presentation'], how='left')\n",
    "      .merge(courses, on=['code_module','code_presentation'], how='left')\n",
    ")\n",
    "\n",
    "# Derived registration and engagement ratios\n",
    "if 'date_registration' in final_data.columns:\n",
    "    final_data['registered_early_flag'] = (final_data['date_registration'] < 0).astype(int)\n",
    "    final_data['registration_lead_days'] = (-final_data['date_registration']).clip(lower=0)\n",
    "\n",
    "if 'date_unregistration' in final_data.columns:\n",
    "    final_data['unregistered_flag'] = final_data['date_unregistration'].notna().astype(int)\n",
    "\n",
    "if 'module_presentation_length' in final_data.columns:\n",
    "    final_data['active_days_ratio'] = final_data['active_days'] / final_data['module_presentation_length'].replace(0, np.nan)\n",
    "    final_data['active_days_ratio'] = final_data['active_days_ratio'].fillna(0)\n",
    "\n",
    "# Drop rows with missing values in this feature list\n",
    "feature_cols = [\n",
    "    'total_sum_click','active_days','early_sum_click','early_active_days','early_click_ratio',\n",
    "    'clicks_per_active_day','early_clicks_per_active_day','early_active_ratio',\n",
    "    'distinct_sites','distinct_activity_types','activity_entropy','top_type_share',\n",
    "    'log1p_total_sum_click','log1p_early_sum_click','exam_weighted','non_exam_weighted',\n",
    "    'mean_score','late_submissions','overall_grade','unregistered_flag',\n",
    "    'registered_early_flag','registration_lead_days','active_days_ratio'\n",
    "]\n",
    "\n",
    "row_na_cols = [c for c in feature_cols if c in final_data.columns]\n",
    "if row_na_cols:\n",
    "    before = len(final_data)\n",
    "    final_data = final_data.dropna(subset=row_na_cols)\n",
    "    print('Dropped rows with NA in feature list:', before - len(final_data))\n",
    "\n",
    "# remove zero-activity rows\n",
    "zero_cols = [c for c in ['total_sum_click','active_days','early_sum_click','early_active_days','distinct_sites'] if c in final_data.columns]\n",
    "if zero_cols:\n",
    "    zero_mask = (final_data[zero_cols].sum(axis=1) == 0)\n",
    "    final_data = final_data.loc[~zero_mask].copy()\n",
    "\n",
    "# drop leakage features\n",
    "LEAKAGE_FEATURES = [\n",
    "    'overall_grade','exam_weighted','non_exam_weighted','mean_score','late_submissions',\n",
    "    'final_result','final_result_upd'\n",
    "]\n",
    "leak_drop = [c for c in LEAKAGE_FEATURES if c in final_data.columns]\n",
    "if leak_drop:\n",
    "    final_data = final_data.drop(columns=leak_drop)\n",
    "    print('Dropped leakage features:', leak_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b91b8cff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T08:07:46.584071Z",
     "iopub.status.busy": "2026-02-01T08:07:46.584071Z",
     "iopub.status.idle": "2026-02-01T08:07:47.396560Z",
     "shell.execute_reply": "2026-02-01T08:07:47.396560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LEAKAGE AUDIT\n",
      "======================================================================\n",
      "Suspicious columns (name-based):\n",
      "['final_result_upd_numeric', 'unregistered_flag']\n",
      "\n",
      "Top 15 single-feature AUCs:\n",
      "('active_days', 0.8346724183825532, 283, 'float64')\n",
      "('active_days_ratio', 0.8327413027489992, 1619, 'float64')\n",
      "('clicks_type_homepage', 0.7984884623869734, 1514, 'float64')\n",
      "('total_sum_click', 0.7957161310727129, 5252, 'float64')\n",
      "('log1p_total_sum_click', 0.7957161310727129, 5252, 'float64')\n",
      "('distinct_sites', 0.7600020139394409, 324, 'float64')\n",
      "('clicks_type_resource', 0.7211647788097261, 375, 'float64')\n",
      "('clicks_type_forumng', 0.7202220272763901, 1984, 'float64')\n",
      "('clicks_type_oucontent', 0.6989721615598872, 2653, 'float64')\n",
      "('clicks_type_subpage', 0.6818249446526621, 944, 'float64')\n",
      "('clicks_type_url', 0.6745309029835852, 286, 'float64')\n",
      "('clicks_type_quiz', 0.6683192395134292, 1865, 'float64')\n",
      "('distinct_activity_types', 0.64678900300169, 15, 'float64')\n",
      "('early_sum_click', 0.6197310411109083, 1427, 'float64')\n",
      "('log1p_early_sum_click', 0.6197310411109083, 1427, 'float64')\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- leakage audit (heuristics) ---\n",
    "print('='*70)\n",
    "print('LEAKAGE AUDIT')\n",
    "print('='*70)\n",
    "\n",
    "# Columns that often encode the label directly\n",
    "suspicious = [\n",
    "    c for c in final_data.columns\n",
    "    if any(k in c.lower() for k in ['final', 'result', 'grade', 'score', 'withdraw', 'unregister'])\n",
    "]\n",
    "print('Suspicious columns (name-based):')\n",
    "print(suspicious)\n",
    "\n",
    "# Single-feature AUC scan (rough signal)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# target is still present in final_data at this stage\n",
    "_tmp_target = 'final_result_upd_numeric'\n",
    "if _tmp_target in final_data.columns:\n",
    "    y = final_data[_tmp_target].astype(int)\n",
    "else:\n",
    "    y = None\n",
    "\n",
    "auc_rank = []\n",
    "if y is not None:\n",
    "    for col in final_data.columns:\n",
    "        if col == _tmp_target:\n",
    "            continue\n",
    "        s = final_data[col]\n",
    "        if s.isna().all() or s.nunique(dropna=True) <= 1:\n",
    "            continue\n",
    "        if s.dtype.kind in 'ifc':\n",
    "            x = s\n",
    "        else:\n",
    "            x = pd.Series(pd.factorize(s)[0], index=s.index)\n",
    "        try:\n",
    "            auc = roc_auc_score(y, x)\n",
    "        except Exception:\n",
    "            continue\n",
    "        auc_rank.append((col, auc, int(s.nunique(dropna=True)), str(s.dtype)))\n",
    "\n",
    "    auc_rank = sorted(auc_rank, key=lambda x: x[1], reverse=True)\n",
    "    print()\n",
    "    print('Top 15 single-feature AUCs:')\n",
    "    for row in auc_rank[:15]:\n",
    "        print(row)\n",
    "print('='*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d1929bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T08:07:47.398777Z",
     "iopub.status.busy": "2026-02-01T08:07:47.398777Z",
     "iopub.status.idle": "2026-02-01T08:08:01.794332Z",
     "shell.execute_reply": "2026-02-01T08:08:01.794332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Initializing H2O\n",
      "======================================================================\n",
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) 64-Bit Server VM (build 17.0.12+8-LTS-286, mixed mode, sharing)\n",
      "  Starting server from C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\THANMI~1\\AppData\\Local\\Temp\\tmpw9pmr085\n",
      "  JVM stdout: C:\\Users\\THANMI~1\\AppData\\Local\\Temp\\tmpw9pmr085\\h2o_Than_Minh_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\THANMI~1\\AppData\\Local\\Temp\\tmpw9pmr085\\h2o_Than_Minh_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-2.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-2 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-2 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-2 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-2 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-2 .h2o-table th,\n",
       "#h2o-table-2 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-2 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-2\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Asia/Ho_Chi_Minh</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.46.0.9</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>2 months and 8 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_Than_Minh_oi4er4</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>6 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>20</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>20</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.11.9 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  --------------------------------\n",
       "H2O_cluster_uptime:         01 secs\n",
       "H2O_cluster_timezone:       Asia/Ho_Chi_Minh\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.46.0.9\n",
       "H2O_cluster_version_age:    2 months and 8 days\n",
       "H2O_cluster_name:           H2O_from_python_Than_Minh_oi4er4\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    6 Gb\n",
       "H2O_cluster_total_cores:    20\n",
       "H2O_cluster_allowed_cores:  20\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.11.9 final\n",
       "--------------------------  --------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "H2O cluster information:\n",
      "  Cluster name: H2O_from_python_Than_Minh_oi4er4\n",
      "  H2O version: 3.46.0.9\n",
      "  Cluster size: 1\n",
      "\n",
      "H2O initialized successfully!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import h2o\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Enable GPU for XGBoost (will be used in standalone training)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "print('='*70)\n",
    "print('Initializing H2O')\n",
    "print('='*70)\n",
    "\n",
    "# Initialize H2O - this starts a local H2O server\n",
    "h2o.init(\n",
    "    max_mem_size='6G',  # Allocate 6GB for H2O\n",
    "    verbose=True,\n",
    "    log_level='INFO',\n",
    "    log_dir='./h2o_logs',\n",
    "    nthreads=-1  # Use all CPU threads\n",
    ")\n",
    "\n",
    "print('\\nH2O cluster information:')\n",
    "print(f'  Cluster name: {h2o.cluster().cloud_name}')\n",
    "print(f'  H2O version: {h2o.__version__}')\n",
    "print(f'  Cluster size: {h2o.cluster().cloud_size}')\n",
    "print('\\nH2O initialized successfully!')\n",
    "print('='*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "h2o_conversion",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T08:08:01.796344Z",
     "iopub.status.busy": "2026-02-01T08:08:01.796344Z",
     "iopub.status.idle": "2026-02-01T08:08:12.549692Z",
     "shell.execute_reply": "2026-02-01T08:08:12.549692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Converting data to H2O format\n",
      "======================================================================\n",
      "\n",
      "Dataset shape: (25766, 55)\n",
      "Columns: 55\n",
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "\n",
      "H2O Frame created:\n",
      "  Rows: 25766\n",
      "  Columns: 55\n",
      "\n",
      "Target: final_result_upd_numeric\n",
      "Features: 54\n",
      "  First 5: ['code_module', 'code_presentation', 'id_student', 'gender', 'region']\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Convert pandas DataFrame to H2O Frame\n",
    "print('='*70)\n",
    "print('Converting data to H2O format')\n",
    "print('='*70)\n",
    "\n",
    "# Import H2O if not already\n",
    "from h2o.estimators import H2OGradientBoostingEstimator, H2OGeneralizedLinearEstimator, H2ORandomForestEstimator\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "print(f'\\nDataset shape: {final_data.shape}')\n",
    "print(f'Columns: {len(final_data.columns)}')\n",
    "\n",
    "# Convert to H2O Frame\n",
    "h2o_data = h2o.H2OFrame(final_data)\n",
    "\n",
    "print(f'\\nH2O Frame created:')\n",
    "print(f'  Rows: {h2o_data.nrow}')\n",
    "print(f'  Columns: {h2o_data.ncol}')\n",
    "\n",
    "# Define target and features\n",
    "target = 'final_result_upd_numeric'\n",
    "\n",
    "# Convert target to factor for classification\n",
    "h2o_data[target] = h2o_data[target].asfactor()\n",
    "\n",
    "# Get feature names (all columns except target)\n",
    "features = [col for col in h2o_data.columns if col != target]\n",
    "\n",
    "print(f'\\nTarget: {target}')\n",
    "print(f'Features: {len(features)}')\n",
    "print(f'  First 5: {features[:5]}')\n",
    "print('='*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a8c2c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE LIST (model inputs)\n",
      "Count: 40\n",
      "['code_module', 'code_presentation', 'gender', 'region', 'highest_education', 'imd_band', 'age_band', 'num_of_prev_attempts', 'studied_credits', 'disability', 'clicks_per_active_day', 'clicks_type_dataplus', 'clicks_type_dualpane', 'clicks_type_externalquiz', 'clicks_type_folder', 'clicks_type_forumng', 'clicks_type_glossary', 'clicks_type_homepage', 'clicks_type_htmlactivity', 'clicks_type_oucollaborate', 'clicks_type_oucontent', 'clicks_type_ouelluminate', 'clicks_type_ouwiki', 'clicks_type_page', 'clicks_type_questionnaire', 'clicks_type_quiz', 'clicks_type_repeatactivity', 'clicks_type_resource', 'clicks_type_sharedsubpage', 'clicks_type_subpage', 'clicks_type_url', 'distinct_activity_types', 'activity_entropy', 'top_type_share', 'has_exam', 'has_non_exam', 'date_registration', 'module_presentation_length', 'registered_early_flag', 'registration_lead_days']\n",
      "Saved: results/feature_list.csv\n"
     ]
    }
   ],
   "source": [
    "# Limit features to a curated whitelist\n",
    "WHITELIST_FEATURES = ['gender', 'region', 'highest_education', 'imd_band', 'age_band', 'disability', 'num_of_prev_attempts', 'studied_credits', 'code_module', 'code_presentation', 'module_presentation_length', 'has_exam', 'has_non_exam', 'registered_early_flag', 'registration_lead_days', 'date_registration', 'clicks_type_dataplus', 'clicks_type_dualpane', 'clicks_type_externalquiz', 'clicks_type_folder', 'clicks_type_forumng', 'clicks_type_glossary', 'clicks_type_homepage', 'clicks_type_htmlactivity', 'clicks_type_oucollaborate', 'clicks_type_oucontent', 'clicks_type_ouelluminate', 'clicks_type_ouwiki', 'clicks_type_page', 'clicks_type_questionnaire', 'clicks_type_quiz', 'clicks_type_repeatactivity', 'clicks_type_resource', 'clicks_type_sharedsubpage', 'clicks_type_subpage', 'clicks_type_url', 'distinct_activity_types', 'activity_entropy', 'top_type_share', 'clicks_per_active_day']\n",
    "\n",
    "features = [f for f in features if f in WHITELIST_FEATURES]\n",
    "print('FEATURE LIST (model inputs)')\n",
    "print(f'Count: {len(features)}')\n",
    "print(features)\n",
    "\n",
    "# Save for inspection\n",
    "import pandas as pd\n",
    "pd.DataFrame({'feature': features}).to_csv('results/feature_list.csv', index=False)\n",
    "print('Saved: results/feature_list.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "train_test_split",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T08:08:12.559547Z",
     "iopub.status.busy": "2026-02-01T08:08:12.558546Z",
     "iopub.status.idle": "2026-02-01T08:08:33.232285Z",
     "shell.execute_reply": "2026-02-01T08:08:33.232285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Creating train/validation/test splits\n",
      "======================================================================\n",
      "\n",
      "Data splits:\n",
      "  Training: 18095 rows (70.2%)\n",
      "  Validation: 3841 rows (14.9%)\n",
      "  Test: 3830 rows (14.9%)\n",
      "\n",
      "Target distribution in training set:\n",
      "  final_result_upd_numeric    Count\n",
      "                         0     7382\n",
      "                         1    10713\n",
      "[2 rows x 2 columns]\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "print('='*70)\n",
    "print('Creating train/validation/test splits')\n",
    "print('='*70)\n",
    "\n",
    "# Split: 70% train, 15% valid, 15% test\n",
    "train, valid, test = h2o_data.split_frame(ratios=[0.7, 0.15], seed=42)\n",
    "\n",
    "print(f'\\nData splits:')\n",
    "print(f'  Training: {train.nrow} rows ({train.nrow/h2o_data.nrow*100:.1f}%)')\n",
    "print(f'  Validation: {valid.nrow} rows ({valid.nrow/h2o_data.nrow*100:.1f}%)')\n",
    "print(f'  Test: {test.nrow} rows ({test.nrow/h2o_data.nrow*100:.1f}%)')\n",
    "\n",
    "# Check class distribution\n",
    "print(f'\\nTarget distribution in training set:')\n",
    "print(train[target].table())\n",
    "\n",
    "print('='*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "baseline_models",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T08:08:33.234290Z",
     "iopub.status.busy": "2026-02-01T08:08:33.234290Z",
     "iopub.status.idle": "2026-02-01T08:08:56.174499Z",
     "shell.execute_reply": "2026-02-01T08:08:56.174499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Training Baseline Models\n",
      "======================================================================\n",
      "\n",
      "*** Training GLM ***\n",
      "glm Model Build progress: |██████████████████████████████████████████████████████| (done) 100%\n",
      "  AUC: 0.9127\n",
      "\n",
      "*** Training GBM ***\n",
      "gbm Model Build progress: |██████████████████████████████████████████████████████| (done) 100%\n",
      "  AUC: 0.9353\n",
      "\n",
      "*** Training DRF ***\n",
      "drf Model Build progress: |██████████████████████████████████████████████████████| (done) 100%\n",
      "  AUC: 0.9246\n",
      "\n",
      "======================================================================\n",
      "Baseline models completed!\n",
      "Results saved to: results/model_logs_baseline_20260201_163020.csv\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Train baseline models\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print('='*70)\n",
    "print('Training Baseline Models')\n",
    "print('='*70)\n",
    "\n",
    "baseline_results = []\n",
    "\n",
    "# GLM\n",
    "print('\\n*** Training GLM ***')\n",
    "glm = H2OGeneralizedLinearEstimator(family='binomial', seed=42)\n",
    "glm.train(x=features, y=target, training_frame=train, validation_frame=valid)\n",
    "perf_glm = glm.model_performance(valid=True)\n",
    "print(f'  AUC: {perf_glm.auc():.4f}')\n",
    "\n",
    "baseline_results.append({\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'source': 'baseline',\n",
    "    'model_type': 'GLM',\n",
    "    'model_name': 'GLM_baseline',\n",
    "    'auc': perf_glm.auc(),\n",
    "    'accuracy': perf_glm.accuracy()[0][1] if perf_glm.accuracy() else None,\n",
    "    'f1_score': perf_glm.F1()[0][1] if perf_glm.F1() else None,\n",
    "    'precision': perf_glm.precision()[0][1] if perf_glm.precision() else None,\n",
    "    'recall': perf_glm.recall()[0][1] if perf_glm.recall() else None,\n",
    "    'logloss': perf_glm.logloss()\n",
    "})\n",
    "\n",
    "# GBM\n",
    "print('\\n*** Training GBM ***')\n",
    "gbm = H2OGradientBoostingEstimator(ntrees=100, max_depth=5, learn_rate=0.1, seed=42)\n",
    "gbm.train(x=features, y=target, training_frame=train, validation_frame=valid)\n",
    "perf_gbm = gbm.model_performance(valid=True)\n",
    "print(f'  AUC: {perf_gbm.auc():.4f}')\n",
    "\n",
    "baseline_results.append({\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'source': 'baseline',\n",
    "    'model_type': 'GBM',\n",
    "    'model_name': 'GBM_baseline',\n",
    "    'auc': perf_gbm.auc(),\n",
    "    'accuracy': perf_gbm.accuracy()[0][1] if perf_gbm.accuracy() else None,\n",
    "    'f1_score': perf_gbm.F1()[0][1] if perf_gbm.F1() else None,\n",
    "    'precision': perf_gbm.precision()[0][1] if perf_gbm.precision() else None,\n",
    "    'recall': perf_gbm.recall()[0][1] if perf_gbm.recall() else None,\n",
    "    'logloss': perf_gbm.logloss()\n",
    "})\n",
    "\n",
    "# DRF\n",
    "print('\\n*** Training DRF ***')\n",
    "drf = H2ORandomForestEstimator(ntrees=100, max_depth=10, seed=42)\n",
    "drf.train(x=features, y=target, training_frame=train, validation_frame=valid)\n",
    "perf_drf = drf.model_performance(valid=True)\n",
    "print(f'  AUC: {perf_drf.auc():.4f}')\n",
    "\n",
    "baseline_results.append({\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'source': 'baseline',\n",
    "    'model_type': 'DRF',\n",
    "    'model_name': 'DRF_baseline',\n",
    "    'auc': perf_drf.auc(),\n",
    "    'accuracy': perf_drf.accuracy()[0][1] if perf_drf.accuracy() else None,\n",
    "    'f1_score': perf_drf.F1()[0][1] if perf_drf.F1() else None,\n",
    "    'precision': perf_drf.precision()[0][1] if perf_drf.precision() else None,\n",
    "    'recall': perf_drf.recall()[0][1] if perf_drf.recall() else None,\n",
    "    'logloss': perf_drf.logloss()\n",
    "})\n",
    "\n",
    "# Save baseline results\n",
    "baseline_results = pd.DataFrame(baseline_results)\n",
    "baseline_results.to_csv(f'results/model_logs_baseline_{timestamp_str}.csv', index=False)\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('Baseline models completed!')\n",
    "print(f'Results saved to: results/model_logs_baseline_{timestamp_str}.csv')\n",
    "print('='*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e6b280",
   "metadata": {},
   "source": [
    "## 4. H2O AutoML\n",
    "\n",
    "We run AutoML to test multiple model families and return the best models ranked by the selected metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8a8d23",
   "metadata": {},
   "source": [
    "## 4. Baseline Models\n",
    "\n",
    "We first train a few standard models to establish a baseline before running AutoML. This gives a clear point of comparison for later tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5a330fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T08:08:56.176521Z",
     "iopub.status.busy": "2026-02-01T08:08:56.176521Z",
     "iopub.status.idle": "2026-02-01T08:10:05.883885Z",
     "shell.execute_reply": "2026-02-01T08:10:05.882801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "H2O AutoML - COMPREHENSIVE RUN\n",
      "======================================================================\n",
      "Training samples: 18095, Validation samples: 3841\n",
      "Features: 40\n",
      "\n",
      "Configuration:\n",
      "  - Max models: 20\n",
      "  - Max runtime: 18000 seconds\n",
      "  - Algorithms: GLM, GBM, DRF, DeepLearning, StackedEnsemble\n",
      "  - Excluded: XGBoost (will train separately)\n",
      "======================================================================\n",
      "\n",
      "Starting at: 16:30:31\n",
      "\n",
      "Training H2O AutoML...\n",
      "----------------------------------------------------------------------\n",
      "AutoML progress: |\n",
      "16:30:31.668: Project: AutoML_Main\n",
      "16:30:31.669: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n",
      "16:30:31.669: Setting stopping tolerance adaptively based on the training frame: 0.007433968324597509\n",
      "16:30:31.669: Build control seed: 42\n",
      "16:30:31.669: training frame: Frame key: AutoML_1_20260201_163031_training_py_9_sid_80d6    cols: 55    rows: 18095  chunks: 3    size: 2319123  checksum: -2629140605513702480\n",
      "16:30:31.669: validation frame: Frame key: py_10_sid_80d6    cols: 55    rows: 3841  chunks: 3    size: 519693  checksum: -7633833196436437242\n",
      "16:30:31.669: leaderboard frame: NULL\n",
      "16:30:31.669: blending frame: NULL\n",
      "16:30:31.669: response column: final_result_upd_numeric\n",
      "16:30:31.669: fold column: null\n",
      "16:30:31.669: weights column: null\n",
      "16:30:31.682: Loading execution steps: [{XGBoost : [def_2 (1g, 10w), def_1 (2g, 10w), def_3 (3g, 10w), grid_1 (4g, 90w), lr_search (7g, 30w)]}, {GLM : [def_1 (1g, 10w)]}, {DRF : [def_1 (2g, 10w), XRT (3g, 10w)]}, {GBM : [def_5 (1g, 10w), def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w), def_1 (3g, 10w), grid_1 (4g, 60w), lr_annealing (7g, 10w)]}, {DeepLearning : [def_1 (3g, 10w), grid_1 (4g, 30w), grid_2 (5g, 30w), grid_3 (5g, 30w)]}, {completion : [resume_best_grids (6g, 60w)]}, {StackedEnsemble : [monotonic (9g, 10w), best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
      "16:30:31.715: Disabling Algo: XGBoost as requested by the user.\n",
      "16:30:31.716: AutoML job created: 2026.02.01 16:30:31.657\n",
      "16:30:31.717: AutoML build started: 2026.02.01 16:30:31.717\n",
      "16:30:31.721: AutoML: starting GLM_1_AutoML_1_20260201_163031 model training\n",
      "\n",
      "███\n",
      "16:30:34.676: New leader: GLM_1_AutoML_1_20260201_163031, auc: 0.9116399079422087\n",
      "16:30:34.678: AutoML: starting GBM_1_AutoML_1_20260201_163031 model training\n",
      "\n",
      "█\n",
      "16:30:41.908: New leader: GBM_1_AutoML_1_20260201_163031, auc: 0.9327925735988526\n",
      "16:30:41.910: AutoML: starting DRF_1_AutoML_1_20260201_163031 model training\n",
      "\n",
      "███\n",
      "16:30:51.566: AutoML: starting GBM_2_AutoML_1_20260201_163031 model training\n",
      "\n",
      "█\n",
      "16:30:56.115: AutoML: starting GBM_3_AutoML_1_20260201_163031 model training\n",
      "\n",
      "█\n",
      "16:31:00.423: AutoML: starting GBM_4_AutoML_1_20260201_163031 model training\n",
      "\n",
      "██\n",
      "16:31:05.823: AutoML: starting XRT_1_AutoML_1_20260201_163031 model training\n",
      "\n",
      "███\n",
      "16:31:16.761: AutoML: starting GBM_5_AutoML_1_20260201_163031 model training\n",
      "\n",
      "█\n",
      "16:31:21.495: New leader: GBM_5_AutoML_1_20260201_163031, auc: 0.9340886742225919\n",
      "16:31:21.499: AutoML: starting DeepLearning_1_AutoML_1_20260201_163031 model training\n",
      "\n",
      "█\n",
      "16:31:25.410: AutoML: starting GBM_grid_1_AutoML_1_20260201_163031 hyperparameter search\n",
      "\n",
      "███████\n",
      "16:32:06.587: AutoML: starting DeepLearning_grid_1_AutoML_1_20260201_163031 hyperparameter search\n",
      "\n",
      "█████████████████████████████████\n",
      "16:39:04.392: AutoML: starting DeepLearning_grid_2_AutoML_1_20260201_163031 hyperparameter search\n",
      "\n",
      "█████\n",
      "16:46:02.157: AutoML: starting DeepLearning_grid_3_AutoML_1_20260201_163031 hyperparameter search\n",
      "\n",
      "█\n",
      "16:53:46.652: No base models, due to timeouts or the exclude_algos option. Skipping StackedEnsemble 'monotonic'.\n",
      "16:53:46.662: AutoML: starting StackedEnsemble_BestOfFamily_1_AutoML_1_20260201_163031 model training\n",
      "\n",
      "\n",
      "16:53:48.682: New leader: StackedEnsemble_BestOfFamily_1_AutoML_1_20260201_163031, auc: 0.9356453859083337\n",
      "16:53:48.683: AutoML: starting StackedEnsemble_AllModels_1_AutoML_1_20260201_163031 model training\n",
      "\n",
      "█| (done) 100%\n",
      "\n",
      "16:53:52.49: New leader: StackedEnsemble_AllModels_1_AutoML_1_20260201_163031, auc: 0.9367440113765516\n",
      "16:53:52.50: Actual modeling steps: [{GLM : [def_1 (1g, 10w)]}, {GBM : [def_5 (1g, 10w)]}, {DRF : [def_1 (2g, 10w)]}, {GBM : [def_2 (2g, 10w), def_3 (2g, 10w), def_4 (2g, 10w)]}, {DRF : [XRT (3g, 10w)]}, {GBM : [def_1 (3g, 10w)]}, {DeepLearning : [def_1 (3g, 10w)]}, {GBM : [grid_1 (4g, 60w)]}, {DeepLearning : [grid_1 (4g, 30w), grid_2 (5g, 30w), grid_3 (5g, 30w)]}, {StackedEnsemble : [best_of_family_xglm (10g, 10w), all_xglm (10g, 10w)]}]\n",
      "16:53:52.50: AutoML build stopped: 2026.02.01 16:53:52.50\n",
      "16:53:52.50: AutoML build done: built 20 models\n",
      "16:53:52.50: AutoML duration: 23 min 20.333 sec\n",
      "\n",
      "\n",
      "======================================================================\n",
      "H2O AutoML COMPLETED\n",
      "======================================================================\n",
      "Completed at: 16:53:53\n",
      "Duration: 0h 23m\n",
      "Models trained: 22\n",
      "\n",
      "*** Top 10 Models (by AUC) ***\n",
      "model_id                                                      auc    logloss\n",
      "StackedEnsemble_AllModels_1_AutoML_1_20260201_163031     0.936744   0.280542\n",
      "StackedEnsemble_BestOfFamily_1_AutoML_1_20260201_163031  0.935645   0.284134\n",
      "GBM_5_AutoML_1_20260201_163031                           0.934089   0.289918\n",
      "GBM_1_AutoML_1_20260201_163031                           0.932793   0.291233\n",
      "GBM_2_AutoML_1_20260201_163031                           0.932725   0.292509\n",
      "GBM_grid_1_AutoML_1_20260201_163031_model_1              0.932039   0.29474\n",
      "GBM_4_AutoML_1_20260201_163031                           0.931457   0.299497\n",
      "GBM_grid_1_AutoML_1_20260201_163031_model_5              0.930761   0.299624\n",
      "GBM_grid_1_AutoML_1_20260201_163031_model_4              0.930676   0.306098\n",
      "GBM_3_AutoML_1_20260201_163031                           0.930264   0.296906\n",
      "[10 rows x 3 columns]\n",
      "\n",
      "\n",
      "H2O AutoML run completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# H2O AutoML - Single Comprehensive Run\n",
    "# Excludes XGBoost (trained separately)\n",
    "\n",
    "if RUN_AUTOML:\n",
    "    print('='*70)\n",
    "    print('H2O AutoML - COMPREHENSIVE RUN')\n",
    "    print('='*70)\n",
    "    print(f'Training samples: {train.nrow}, Validation samples: {valid.nrow}')\n",
    "    print(f'Features: {len(features)}')\n",
    "    print()\n",
    "    print('Configuration:')\n",
    "    print(f'  - Max models: {AUTOML_MAX_MODELS}')\n",
    "    print(f'  - Max runtime: {AUTOML_MAX_RUNTIME_SECS} seconds')\n",
    "    print('  - Algorithms: GLM, GBM, DRF, DeepLearning, StackedEnsemble')\n",
    "    print('  - Excluded: XGBoost (will train separately)')\n",
    "    print('='*70)\n",
    "\n",
    "    from datetime import datetime\n",
    "    start_time = datetime.now()\n",
    "    print()\n",
    "    print(f\"Starting at: {start_time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "    aml = H2OAutoML(\n",
    "        max_runtime_secs=AUTOML_MAX_RUNTIME_SECS,\n",
    "        max_models=AUTOML_MAX_MODELS,\n",
    "        balance_classes=True,\n",
    "        sort_metric='AUC',\n",
    "        seed=42,\n",
    "        exclude_algos=['XGBoost'],\n",
    "        verbosity='info',\n",
    "        nfolds=AUTOML_NFOLDS,\n",
    "        project_name='AutoML_Main'\n",
    "    )\n",
    "\n",
    "    print()\n",
    "    print('Training H2O AutoML...')\n",
    "    print('-'*70)\n",
    "    aml.train(x=features, y=target, training_frame=train, validation_frame=valid)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    duration = (end_time - start_time).seconds\n",
    "\n",
    "    print()\n",
    "    print('='*70)\n",
    "    print('H2O AutoML COMPLETED')\n",
    "    print('='*70)\n",
    "    print(f\"Completed at: {end_time.strftime('%H:%M:%S')}\")\n",
    "    print(f'Duration: {duration // 3600}h {(duration % 3600) // 60}m')\n",
    "    print(f'Models trained: {len(aml.leaderboard)}')\n",
    "\n",
    "    print()\n",
    "    print('*** Top 10 Models (by AUC) ***')\n",
    "    print(aml.leaderboard.head(10)[['model_id', 'auc', 'logloss']])\n",
    "\n",
    "    print()\n",
    "    print('H2O AutoML run completed successfully!')\n",
    "else:\n",
    "    aml = None\n",
    "    print('Skipping H2O AutoML (RUN_AUTOML=0)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47818e08",
   "metadata": {},
   "source": [
    "## 5. Evaluate top models\n",
    "\n",
    "We compute Accuracy and F1 for the top models on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea291146",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T08:10:05.886886Z",
     "iopub.status.busy": "2026-02-01T08:10:05.885885Z",
     "iopub.status.idle": "2026-02-01T08:10:11.839731Z",
     "shell.execute_reply": "2026-02-01T08:10:11.839229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "XGBoost Training with Hyperparameter Optimization\n",
      "======================================================================\n",
      "\n",
      "1. Preparing data for XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\h2o\\frame.py:1983: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
      "\n",
      "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Using 32 numeric features (out of 40 total)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\h2o\\frame.py:1983: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
      "\n",
      "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n",
      "\u001b[32m[I 2026-02-01 16:53:54,263]\u001b[0m A new study created in memory with name: xgboost_optimization\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training samples: 18095\n",
      "   Validation samples: 3841\n",
      "   Features: 40\n",
      "\n",
      "2. Running hyperparameter optimization with Optuna...\n",
      "   Target: 15 trials\n",
      "   Optimization method: TPE (Tree-structured Parzen Estimator)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\callback.py:386: UserWarning: [16:53:54] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n",
      "Best trial: 0. Best value: 0.925108:   7%|▋         | 1/15 [00:00<00:08,  1.70it/s]C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\callback.py:386: UserWarning: [16:53:54] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-01 16:53:54,852]\u001b[0m Trial 0 finished with value: 0.9251079476148555 and parameters: {'max_depth': 5, 'learning_rate': 0.2536999076681772, 'n_estimators': 759, 'min_child_weight': 6, 'subsample': 0.6624074561769746, 'colsample_bytree': 0.662397808134481, 'gamma': 0.2904180608409973, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044}. Best is trial 0 with value: 0.9251079476148555.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.930429:  13%|█▎        | 2/15 [00:05<00:39,  3.08s/it]C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\callback.py:386: UserWarning: [16:53:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-01 16:53:59,669]\u001b[0m Trial 1 finished with value: 0.9304286674466655 and parameters: {'max_depth': 8, 'learning_rate': 0.010725209743171996, 'n_estimators': 973, 'min_child_weight': 9, 'subsample': 0.6849356442713105, 'colsample_bytree': 0.6727299868828402, 'gamma': 0.9170225492671691, 'reg_alpha': 1.5212112147976886, 'reg_lambda': 2.6237821581611893}. Best is trial 1 with value: 0.9304286674466655.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.930429:  20%|██        | 3/15 [00:07<00:31,  2.65s/it]C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\callback.py:386: UserWarning: [16:54:01] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-01 16:54:01,821]\u001b[0m Trial 2 finished with value: 0.9298026504873483 and parameters: {'max_depth': 6, 'learning_rate': 0.02692655251486473, 'n_estimators': 651, 'min_child_weight': 2, 'subsample': 0.7168578594140873, 'colsample_bytree': 0.7465447373174767, 'gamma': 2.28034992108518, 'reg_alpha': 3.925879806965068, 'reg_lambda': 0.9983689107917987}. Best is trial 1 with value: 0.9304286674466655.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.930429:  27%|██▋       | 4/15 [00:08<00:19,  1.79s/it]C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\callback.py:386: UserWarning: [16:54:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-01 16:54:02,296]\u001b[0m Trial 3 finished with value: 0.9259772427418242 and parameters: {'max_depth': 7, 'learning_rate': 0.07500118950416987, 'n_estimators': 141, 'min_child_weight': 7, 'subsample': 0.6682096494749166, 'colsample_bytree': 0.6260206371941118, 'gamma': 4.7444276862666666, 'reg_alpha': 4.828160165372797, 'reg_lambda': 4.041986740582305}. Best is trial 1 with value: 0.9304286674466655.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.930429:  33%|███▎      | 5/15 [00:10<00:20,  2.02s/it]C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\callback.py:386: UserWarning: [16:54:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-01 16:54:04,719]\u001b[0m Trial 4 finished with value: 0.9267265960745088 and parameters: {'max_depth': 5, 'learning_rate': 0.013940346079873234, 'n_estimators': 716, 'min_child_weight': 5, 'subsample': 0.6488152939379115, 'colsample_bytree': 0.798070764044508, 'gamma': 0.17194260557609198, 'reg_alpha': 4.546602010393911, 'reg_lambda': 1.2938999080000846}. Best is trial 1 with value: 0.9304286674466655.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.930429:  40%|████      | 6/15 [00:11<00:16,  1.81s/it]C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\callback.py:386: UserWarning: [16:54:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-01 16:54:06,132]\u001b[0m Trial 5 finished with value: 0.9280838619447318 and parameters: {'max_depth': 8, 'learning_rate': 0.028869220380495747, 'n_estimators': 568, 'min_child_weight': 6, 'subsample': 0.6739417822102108, 'colsample_bytree': 0.9878338511058234, 'gamma': 3.8756641168055728, 'reg_alpha': 4.697494707820946, 'reg_lambda': 4.474136752138244}. Best is trial 1 with value: 0.9304286674466655.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.930429:  47%|████▋     | 7/15 [00:12<00:10,  1.36s/it]C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\callback.py:386: UserWarning: [16:54:06] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-01 16:54:06,567]\u001b[0m Trial 6 finished with value: 0.9257370762716659 and parameters: {'max_depth': 7, 'learning_rate': 0.22999586428143728, 'n_estimators': 179, 'min_child_weight': 2, 'subsample': 0.6180909155642152, 'colsample_bytree': 0.7301321323053057, 'gamma': 1.9433864484474102, 'reg_alpha': 1.3567451588694794, 'reg_lambda': 4.143687545759647}. Best is trial 1 with value: 0.9304286674466655.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.930429:  53%|█████▎    | 8/15 [00:13<00:09,  1.39s/it]C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\callback.py:386: UserWarning: [16:54:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-01 16:54:08,019]\u001b[0m Trial 7 finished with value: 0.924697203545751 and parameters: {'max_depth': 5, 'learning_rate': 0.026000059117302653, 'n_estimators': 588, 'min_child_weight': 2, 'subsample': 0.9208787923016158, 'colsample_bytree': 0.6298202574719083, 'gamma': 4.9344346830025865, 'reg_alpha': 3.861223846483287, 'reg_lambda': 0.993578407670862}. Best is trial 1 with value: 0.9304286674466655.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.930429:  60%|██████    | 9/15 [00:14<00:07,  1.22s/it]C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\callback.py:386: UserWarning: [16:54:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-01 16:54:08,856]\u001b[0m Trial 8 finished with value: 0.9279739624469031 and parameters: {'max_depth': 3, 'learning_rate': 0.1601531217136121, 'n_estimators': 736, 'min_child_weight': 8, 'subsample': 0.9085081386743783, 'colsample_bytree': 0.6296178606936361, 'gamma': 1.7923286427213632, 'reg_alpha': 0.5793452976256486, 'reg_lambda': 4.315517129377968}. Best is trial 1 with value: 0.9304286674466655.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.930429:  67%|██████▋   | 10/15 [00:15<00:05,  1.06s/it]C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\callback.py:386: UserWarning: [16:54:09] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-01 16:54:09,556]\u001b[0m Trial 9 finished with value: 0.9260635216268633 and parameters: {'max_depth': 7, 'learning_rate': 0.030816017044468066, 'n_estimators': 157, 'min_child_weight': 4, 'subsample': 0.7300733288106989, 'colsample_bytree': 0.8918424713352255, 'gamma': 3.1877873567760657, 'reg_alpha': 4.436063712881633, 'reg_lambda': 2.3610746258097466}. Best is trial 1 with value: 0.9304286674466655.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.930429:  73%|███████▎  | 11/15 [00:21<00:09,  2.49s/it]C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\callback.py:386: UserWarning: [16:54:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-01 16:54:15,282]\u001b[0m Trial 10 finished with value: 0.9304112702288625 and parameters: {'max_depth': 10, 'learning_rate': 0.010206070557577008, 'n_estimators': 973, 'min_child_weight': 10, 'subsample': 0.8182873120328862, 'colsample_bytree': 0.876098829427658, 'gamma': 1.2772172938259945, 'reg_alpha': 2.020130360093224, 'reg_lambda': 2.396475863679739}. Best is trial 1 with value: 0.9304286674466655.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.930429:  80%|████████  | 12/15 [00:25<00:09,  3.23s/it]C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\callback.py:386: UserWarning: [16:54:20] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-01 16:54:20,209]\u001b[0m Trial 11 finished with value: 0.9303983991165041 and parameters: {'max_depth': 10, 'learning_rate': 0.010290509463842875, 'n_estimators': 971, 'min_child_weight': 10, 'subsample': 0.8153994220705195, 'colsample_bytree': 0.8810796931031994, 'gamma': 0.9996988961166513, 'reg_alpha': 2.317959276605608, 'reg_lambda': 2.417094853247184}. Best is trial 1 with value: 0.9304286674466655.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.930429:  87%|████████▋ | 13/15 [00:30<00:07,  3.56s/it]C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\callback.py:386: UserWarning: [16:54:24] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-01 16:54:24,515]\u001b[0m Trial 12 finished with value: 0.9303998135244557 and parameters: {'max_depth': 10, 'learning_rate': 0.014031806090818524, 'n_estimators': 974, 'min_child_weight': 10, 'subsample': 0.8376702913106802, 'colsample_bytree': 0.883574864264175, 'gamma': 0.9081932656466285, 'reg_alpha': 2.371413413327705, 'reg_lambda': 3.191925527471864}. Best is trial 1 with value: 0.9304286674466655.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.930429:  93%|█████████▎| 14/15 [00:31<00:02,  2.89s/it]C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\callback.py:386: UserWarning: [16:54:25] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-01 16:54:25,856]\u001b[0m Trial 13 finished with value: 0.9294971383698326 and parameters: {'max_depth': 9, 'learning_rate': 0.0651739755127525, 'n_estimators': 887, 'min_child_weight': 9, 'subsample': 0.7667698433310002, 'colsample_bytree': 0.9500805503504125, 'gamma': 1.2077746649016994, 'reg_alpha': 1.5456697915226405, 'reg_lambda': 1.764674394385593}. Best is trial 1 with value: 0.9304286674466655.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.930429: 100%|██████████| 15/15 [00:35<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2026-02-01 16:54:29,393]\u001b[0m Trial 14 finished with value: 0.9297381534847616 and parameters: {'max_depth': 9, 'learning_rate': 0.010230091818268105, 'n_estimators': 851, 'min_child_weight': 8, 'subsample': 0.9979843515077919, 'colsample_bytree': 0.8124496121405841, 'gamma': 2.7939929152165717, 'reg_alpha': 3.059041469985783, 'reg_lambda': 3.2616637833365636}. Best is trial 1 with value: 0.9304286674466655.\u001b[0m\n",
      "\n",
      "3. Best hyperparameters found:\n",
      "   max_depth: 8\n",
      "   learning_rate: 0.010725209743171996\n",
      "   n_estimators: 973\n",
      "   min_child_weight: 9\n",
      "   subsample: 0.6849356442713105\n",
      "   colsample_bytree: 0.6727299868828402\n",
      "   gamma: 0.9170225492671691\n",
      "   reg_alpha: 1.5212112147976886\n",
      "   reg_lambda: 2.6237821581611893\n",
      "   Best validation AUC: 0.9304\n",
      "\n",
      "4. Training final XGBoost model with best parameters...\n",
      "[0]\ttrain-auc:0.86570\tvalidation-auc:0.85459\n",
      "[10]\ttrain-auc:0.91430\tvalidation-auc:0.90170\n",
      "[20]\ttrain-auc:0.92062\tvalidation-auc:0.90936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\ttrain-auc:0.92215\tvalidation-auc:0.91053\n",
      "[40]\ttrain-auc:0.92419\tvalidation-auc:0.91213\n",
      "[50]\ttrain-auc:0.92633\tvalidation-auc:0.91387\n",
      "[60]\ttrain-auc:0.92735\tvalidation-auc:0.91424\n",
      "[70]\ttrain-auc:0.92875\tvalidation-auc:0.91511\n",
      "[80]\ttrain-auc:0.93011\tvalidation-auc:0.91594\n",
      "[90]\ttrain-auc:0.93136\tvalidation-auc:0.91661\n",
      "[100]\ttrain-auc:0.93229\tvalidation-auc:0.91729\n",
      "[110]\ttrain-auc:0.93325\tvalidation-auc:0.91763\n",
      "[120]\ttrain-auc:0.93418\tvalidation-auc:0.91813\n",
      "[130]\ttrain-auc:0.93519\tvalidation-auc:0.91891\n",
      "[140]\ttrain-auc:0.93610\tvalidation-auc:0.91956\n",
      "[150]\ttrain-auc:0.93682\tvalidation-auc:0.91995\n",
      "[160]\ttrain-auc:0.93762\tvalidation-auc:0.92057\n",
      "[170]\ttrain-auc:0.93825\tvalidation-auc:0.92104\n",
      "[180]\ttrain-auc:0.93898\tvalidation-auc:0.92147\n",
      "[190]\ttrain-auc:0.93969\tvalidation-auc:0.92200\n",
      "[200]\ttrain-auc:0.94046\tvalidation-auc:0.92240\n",
      "[210]\ttrain-auc:0.94117\tvalidation-auc:0.92258\n",
      "[220]\ttrain-auc:0.94191\tvalidation-auc:0.92295\n",
      "[230]\ttrain-auc:0.94251\tvalidation-auc:0.92339\n",
      "[240]\ttrain-auc:0.94314\tvalidation-auc:0.92380\n",
      "[250]\ttrain-auc:0.94382\tvalidation-auc:0.92422\n",
      "[260]\ttrain-auc:0.94436\tvalidation-auc:0.92442\n",
      "[270]\ttrain-auc:0.94494\tvalidation-auc:0.92477\n",
      "[280]\ttrain-auc:0.94562\tvalidation-auc:0.92510\n",
      "[290]\ttrain-auc:0.94614\tvalidation-auc:0.92553\n",
      "[300]\ttrain-auc:0.94674\tvalidation-auc:0.92577\n",
      "[310]\ttrain-auc:0.94729\tvalidation-auc:0.92587\n",
      "[320]\ttrain-auc:0.94786\tvalidation-auc:0.92616\n",
      "[330]\ttrain-auc:0.94837\tvalidation-auc:0.92642\n",
      "[340]\ttrain-auc:0.94887\tvalidation-auc:0.92667\n",
      "[350]\ttrain-auc:0.94935\tvalidation-auc:0.92680\n",
      "[360]\ttrain-auc:0.94985\tvalidation-auc:0.92699\n",
      "[370]\ttrain-auc:0.95022\tvalidation-auc:0.92706\n",
      "[380]\ttrain-auc:0.95065\tvalidation-auc:0.92736\n",
      "[390]\ttrain-auc:0.95102\tvalidation-auc:0.92749\n",
      "[400]\ttrain-auc:0.95145\tvalidation-auc:0.92769\n",
      "[410]\ttrain-auc:0.95196\tvalidation-auc:0.92791\n",
      "[420]\ttrain-auc:0.95233\tvalidation-auc:0.92797\n",
      "[430]\ttrain-auc:0.95268\tvalidation-auc:0.92805\n",
      "[440]\ttrain-auc:0.95308\tvalidation-auc:0.92810\n",
      "[450]\ttrain-auc:0.95342\tvalidation-auc:0.92816\n",
      "[460]\ttrain-auc:0.95386\tvalidation-auc:0.92827\n",
      "[470]\ttrain-auc:0.95426\tvalidation-auc:0.92831\n",
      "[480]\ttrain-auc:0.95463\tvalidation-auc:0.92840\n",
      "[490]\ttrain-auc:0.95500\tvalidation-auc:0.92852\n",
      "[500]\ttrain-auc:0.95538\tvalidation-auc:0.92860\n",
      "[510]\ttrain-auc:0.95577\tvalidation-auc:0.92865\n",
      "[520]\ttrain-auc:0.95612\tvalidation-auc:0.92879\n",
      "[530]\ttrain-auc:0.95646\tvalidation-auc:0.92888\n",
      "[540]\ttrain-auc:0.95675\tvalidation-auc:0.92895\n",
      "[550]\ttrain-auc:0.95705\tvalidation-auc:0.92895\n",
      "[560]\ttrain-auc:0.95738\tvalidation-auc:0.92898\n",
      "[570]\ttrain-auc:0.95773\tvalidation-auc:0.92893\n",
      "[580]\ttrain-auc:0.95802\tvalidation-auc:0.92895\n",
      "[590]\ttrain-auc:0.95828\tvalidation-auc:0.92899\n",
      "[600]\ttrain-auc:0.95860\tvalidation-auc:0.92908\n",
      "[610]\ttrain-auc:0.95898\tvalidation-auc:0.92924\n",
      "[620]\ttrain-auc:0.95928\tvalidation-auc:0.92928\n",
      "[630]\ttrain-auc:0.95958\tvalidation-auc:0.92930\n",
      "[640]\ttrain-auc:0.95983\tvalidation-auc:0.92942\n",
      "[650]\ttrain-auc:0.96017\tvalidation-auc:0.92947\n",
      "[660]\ttrain-auc:0.96045\tvalidation-auc:0.92948\n",
      "[670]\ttrain-auc:0.96074\tvalidation-auc:0.92952\n",
      "[680]\ttrain-auc:0.96102\tvalidation-auc:0.92958\n",
      "[690]\ttrain-auc:0.96135\tvalidation-auc:0.92966\n",
      "[700]\ttrain-auc:0.96160\tvalidation-auc:0.92972\n",
      "[710]\ttrain-auc:0.96187\tvalidation-auc:0.92976\n",
      "[720]\ttrain-auc:0.96217\tvalidation-auc:0.92980\n",
      "[730]\ttrain-auc:0.96240\tvalidation-auc:0.92979\n",
      "[740]\ttrain-auc:0.96264\tvalidation-auc:0.92976\n",
      "[750]\ttrain-auc:0.96294\tvalidation-auc:0.92979\n",
      "[760]\ttrain-auc:0.96318\tvalidation-auc:0.92981\n",
      "[770]\ttrain-auc:0.96345\tvalidation-auc:0.92981\n",
      "[780]\ttrain-auc:0.96375\tvalidation-auc:0.92980\n",
      "[790]\ttrain-auc:0.96403\tvalidation-auc:0.92989\n",
      "[800]\ttrain-auc:0.96429\tvalidation-auc:0.92988\n",
      "[810]\ttrain-auc:0.96457\tvalidation-auc:0.92992\n",
      "[820]\ttrain-auc:0.96479\tvalidation-auc:0.92994\n",
      "[830]\ttrain-auc:0.96506\tvalidation-auc:0.92999\n",
      "[840]\ttrain-auc:0.96531\tvalidation-auc:0.93002\n",
      "[850]\ttrain-auc:0.96559\tvalidation-auc:0.93005\n",
      "[860]\ttrain-auc:0.96580\tvalidation-auc:0.93008\n",
      "[870]\ttrain-auc:0.96601\tvalidation-auc:0.93006\n",
      "[880]\ttrain-auc:0.96623\tvalidation-auc:0.93017\n",
      "[890]\ttrain-auc:0.96644\tvalidation-auc:0.93015\n",
      "[900]\ttrain-auc:0.96668\tvalidation-auc:0.93013\n",
      "[910]\ttrain-auc:0.96698\tvalidation-auc:0.93022\n",
      "[920]\ttrain-auc:0.96722\tvalidation-auc:0.93021\n",
      "[930]\ttrain-auc:0.96747\tvalidation-auc:0.93025\n",
      "[940]\ttrain-auc:0.96769\tvalidation-auc:0.93032\n",
      "[950]\ttrain-auc:0.96789\tvalidation-auc:0.93036\n",
      "[960]\ttrain-auc:0.96815\tvalidation-auc:0.93038\n",
      "[970]\ttrain-auc:0.96843\tvalidation-auc:0.93038\n",
      "[972]\ttrain-auc:0.96847\tvalidation-auc:0.93043\n",
      "\n",
      "5. Final XGBoost model evaluation:\n",
      "   AUC: 0.9304\n",
      "   Accuracy: 0.8813\n",
      "   F1 Score: 0.9051\n",
      "   Precision: 0.8724\n",
      "   Recall: 0.9403\n",
      "   Log Loss: 0.3015\n",
      "\n",
      "Model saved to: results/xgboost_best_model.json\n",
      "\n",
      "======================================================================\n",
      "XGBoost optimization completed successfully!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STANDALONE XGBoost with Hyperparameter Optimization\n",
    "# ============================================================\n",
    "\n",
    "if RUN_XGBOOST:\n",
    "    print('='*70)\n",
    "    print('XGBoost Training with Hyperparameter Optimization')\n",
    "    print('='*70)\n",
    "\n",
    "    # Install optuna if needed\n",
    "    try:\n",
    "        import optuna\n",
    "    except ImportError:\n",
    "        !pip install optuna\n",
    "        import optuna\n",
    "\n",
    "    import xgboost as xgb\n",
    "    from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, log_loss\n",
    "\n",
    "    print()\n",
    "    print('1. Preparing data for XGBoost...')\n",
    "    train_pd = train.as_data_frame()\n",
    "    valid_pd = valid.as_data_frame()\n",
    "\n",
    "    numeric_features = [f for f in features if train_pd[f].dtype in ['int64', 'float64']]\n",
    "    print(f'   Using {len(numeric_features)} numeric features (out of {len(features)} total)')\n",
    "\n",
    "    X_train = train_pd[numeric_features]\n",
    "    y_train = train_pd[target].astype(int)\n",
    "    X_valid = valid_pd[numeric_features]\n",
    "    y_valid = valid_pd[target].astype(int)\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "    print(f'   Training samples: {len(X_train)}')\n",
    "    print(f'   Validation samples: {len(X_valid)}')\n",
    "    print(f'   Features: {len(features)}')\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'device': XGB_DEVICE,\n",
    "            'tree_method': XGB_TREE_METHOD,\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "            'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 5),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0, 5),\n",
    "        }\n",
    "\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=params['n_estimators'],\n",
    "            evals=[(dvalid, 'validation')],\n",
    "            early_stopping_rounds=XGB_EARLY_STOPPING_ROUNDS,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "\n",
    "        y_pred_proba = model.predict(dvalid)\n",
    "        auc = roc_auc_score(y_valid, y_pred_proba)\n",
    "        return auc\n",
    "\n",
    "    print()\n",
    "    print('2. Running hyperparameter optimization with Optuna...')\n",
    "    print(f'   Target: {XGB_N_TRIALS} trials')\n",
    "    print('   Optimization method: TPE (Tree-structured Parzen Estimator)')\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction='maximize',\n",
    "        study_name='xgboost_optimization',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42)\n",
    "    )\n",
    "    study.optimize(objective, n_trials=XGB_N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_auc = study.best_value\n",
    "\n",
    "    print()\n",
    "    print('3. Best hyperparameters found:')\n",
    "    for param, value in best_params.items():\n",
    "        print(f'   {param}: {value}')\n",
    "    print(f'   Best validation AUC: {best_auc:.4f}')\n",
    "\n",
    "    print()\n",
    "    print('4. Training final XGBoost model with best parameters...')\n",
    "    final_params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'device': XGB_DEVICE,\n",
    "        'tree_method': XGB_TREE_METHOD,\n",
    "        **best_params\n",
    "    }\n",
    "    n_estimators = final_params.pop('n_estimators')\n",
    "    final_model = xgb.train(\n",
    "        final_params,\n",
    "        dtrain,\n",
    "        num_boost_round=n_estimators,\n",
    "        evals=[(dtrain, 'train'), (dvalid, 'validation')],\n",
    "        early_stopping_rounds=XGB_EARLY_STOPPING_ROUNDS,\n",
    "        verbose_eval=10\n",
    "    )\n",
    "\n",
    "    print()\n",
    "    print('5. Final XGBoost model evaluation:')\n",
    "    y_pred_proba = final_model.predict(dvalid)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "    xgb_metrics = {\n",
    "        'model_type': 'XGBoost_Optimized',\n",
    "        'model_name': 'XGBoost_Optuna_Best',\n",
    "        'auc': roc_auc_score(y_valid, y_pred_proba),\n",
    "        'accuracy': accuracy_score(y_valid, y_pred),\n",
    "        'f1_score': f1_score(y_valid, y_pred),\n",
    "        'precision': precision_score(y_valid, y_pred),\n",
    "        'recall': recall_score(y_valid, y_pred),\n",
    "        'logloss': log_loss(y_valid, y_pred_proba),\n",
    "    }\n",
    "\n",
    "    print(f\"   AUC: {xgb_metrics['auc']:.4f}\")\n",
    "    print(f\"   Accuracy: {xgb_metrics['accuracy']:.4f}\")\n",
    "    print(f\"   F1 Score: {xgb_metrics['f1_score']:.4f}\")\n",
    "    print(f\"   Precision: {xgb_metrics['precision']:.4f}\")\n",
    "    print(f\"   Recall: {xgb_metrics['recall']:.4f}\")\n",
    "    print(f\"   Log Loss: {xgb_metrics['logloss']:.4f}\")\n",
    "\n",
    "    final_model.save_model('results/xgboost_best_model.json')\n",
    "    print()\n",
    "    print('Model saved to: results/xgboost_best_model.json')\n",
    "    print()\n",
    "    print('='*70)\n",
    "    print('XGBoost optimization completed successfully!')\n",
    "    print('='*70)\n",
    "else:\n",
    "    xgb_metrics = None\n",
    "    print('Skipping XGBoost (RUN_XGBOOST=0)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "561277bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T08:10:11.841735Z",
     "iopub.status.busy": "2026-02-01T08:10:11.841735Z",
     "iopub.status.idle": "2026-02-01T08:12:03.376706Z",
     "shell.execute_reply": "2026-02-01T08:12:03.376706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPREHENSIVE MODEL EVALUATION\n",
      "======================================================================\n",
      "\n",
      "*** Evaluating H2O AutoML Models ***\n",
      "Models from H2O AutoML: 22\n",
      "  Model 1/22: StackedEnsemble_AllModels_1_AutoML_1_20260201_163031\n",
      "  Model 2/22: StackedEnsemble_BestOfFamily_1_AutoML_1_20260201_163031\n",
      "  Model 3/22: GBM_5_AutoML_1_20260201_163031\n",
      "  Model 4/22: GBM_1_AutoML_1_20260201_163031\n",
      "  Model 5/22: GBM_2_AutoML_1_20260201_163031\n",
      "  ... processing 12 models ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Than Minh\\AppData\\Roaming\\Python\\Python311\\site-packages\\h2o\\frame.py:1983: H2ODependencyWarning: Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using multi-thread, install polars and pyarrow and use it as pandas_df = h2o_df.as_data_frame(use_multi_thread=True)\n",
      "\n",
      "  warnings.warn(\"Converting H2O frame to pandas dataframe using single-thread.  For faster conversion using\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model 18/22: DeepLearning_grid_2_AutoML_1_20260201_163031_model_2\n",
      "  Model 19/22: DeepLearning_grid_3_AutoML_1_20260201_163031_model_2\n",
      "  Model 20/22: GBM_grid_1_AutoML_1_20260201_163031_model_3\n",
      "  Model 21/22: XRT_1_AutoML_1_20260201_163031\n",
      "  Model 22/22: GLM_1_AutoML_1_20260201_163031\n",
      "\n",
      "*** Adding XGBoost Model ***\n",
      "  XGBoost AUC: 0.9304\n",
      "\n",
      "======================================================================\n",
      "RESULTS SUMMARY\n",
      "======================================================================\n",
      "Total models evaluated: 26\n",
      "  - Baseline: 3\n",
      "  - H2O AutoML: 22\n",
      "  - XGBoost: 1\n",
      "\n",
      "Files saved:\n",
      "  - results/model_logs_complete_20260201_163020.csv (ALL models)\n",
      "  - results/model_logs_h2o_automl_20260201_163020.csv\n",
      "  - results/model_logs_xgboost_20260201_163020.csv\n",
      "\n",
      "*** TOP 15 MODELS OVERALL (by AUC) ***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall_rank</th>\n",
       "      <th>source</th>\n",
       "      <th>model_type</th>\n",
       "      <th>model_name</th>\n",
       "      <th>auc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>StackedEnsemble</td>\n",
       "      <td>StackedEnsemble_AllModels_1_AutoML_1_20260201_...</td>\n",
       "      <td>0.936405</td>\n",
       "      <td>0.887529</td>\n",
       "      <td>0.911656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_2_AutoML_1_20260201_163031</td>\n",
       "      <td>0.935718</td>\n",
       "      <td>0.887529</td>\n",
       "      <td>0.910512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>baseline</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_baseline</td>\n",
       "      <td>0.935325</td>\n",
       "      <td>0.883364</td>\n",
       "      <td>0.908981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_grid_1_AutoML_1_20260201_163031_model_1</td>\n",
       "      <td>0.934196</td>\n",
       "      <td>0.882583</td>\n",
       "      <td>0.907453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>StackedEnsemble</td>\n",
       "      <td>StackedEnsemble_BestOfFamily_1_AutoML_1_202602...</td>\n",
       "      <td>0.933759</td>\n",
       "      <td>0.884405</td>\n",
       "      <td>0.909571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_1_AutoML_1_20260201_163031</td>\n",
       "      <td>0.933149</td>\n",
       "      <td>0.882843</td>\n",
       "      <td>0.908833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_5_AutoML_1_20260201_163031</td>\n",
       "      <td>0.932727</td>\n",
       "      <td>0.886228</td>\n",
       "      <td>0.909863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_4_AutoML_1_20260201_163031</td>\n",
       "      <td>0.931311</td>\n",
       "      <td>0.885446</td>\n",
       "      <td>0.911075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_grid_1_AutoML_1_20260201_163031_model_5</td>\n",
       "      <td>0.931226</td>\n",
       "      <td>0.885707</td>\n",
       "      <td>0.910277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_3_AutoML_1_20260201_163031</td>\n",
       "      <td>0.930746</td>\n",
       "      <td>0.884145</td>\n",
       "      <td>0.910264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>XGBoost_Optuna</td>\n",
       "      <td>XGBoost_Optimized</td>\n",
       "      <td>XGBoost_Optimized</td>\n",
       "      <td>0.930429</td>\n",
       "      <td>0.881281</td>\n",
       "      <td>0.905079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_grid_1_AutoML_1_20260201_163031_model_4</td>\n",
       "      <td>0.929151</td>\n",
       "      <td>0.883364</td>\n",
       "      <td>0.908582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_grid_1_AutoML_1_20260201_163031_model_2</td>\n",
       "      <td>0.927931</td>\n",
       "      <td>0.885186</td>\n",
       "      <td>0.910311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>DRF</td>\n",
       "      <td>DRF_1_AutoML_1_20260201_163031</td>\n",
       "      <td>0.927341</td>\n",
       "      <td>0.876855</td>\n",
       "      <td>0.903292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>DeepLearning</td>\n",
       "      <td>DeepLearning_grid_1_AutoML_1_20260201_163031_m...</td>\n",
       "      <td>0.925463</td>\n",
       "      <td>0.874251</td>\n",
       "      <td>0.902507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    overall_rank          source         model_type  \\\n",
       "0              1      H2O_AutoML    StackedEnsemble   \n",
       "1              2      H2O_AutoML                GBM   \n",
       "2              3        baseline                GBM   \n",
       "3              4      H2O_AutoML                GBM   \n",
       "4              5      H2O_AutoML    StackedEnsemble   \n",
       "5              6      H2O_AutoML                GBM   \n",
       "6              7      H2O_AutoML                GBM   \n",
       "7              8      H2O_AutoML                GBM   \n",
       "8              9      H2O_AutoML                GBM   \n",
       "9             10      H2O_AutoML                GBM   \n",
       "10            11  XGBoost_Optuna  XGBoost_Optimized   \n",
       "11            12      H2O_AutoML                GBM   \n",
       "12            13      H2O_AutoML                GBM   \n",
       "13            14      H2O_AutoML                DRF   \n",
       "14            15      H2O_AutoML       DeepLearning   \n",
       "\n",
       "                                           model_name       auc  accuracy  \\\n",
       "0   StackedEnsemble_AllModels_1_AutoML_1_20260201_...  0.936405  0.887529   \n",
       "1                      GBM_2_AutoML_1_20260201_163031  0.935718  0.887529   \n",
       "2                                        GBM_baseline  0.935325  0.883364   \n",
       "3         GBM_grid_1_AutoML_1_20260201_163031_model_1  0.934196  0.882583   \n",
       "4   StackedEnsemble_BestOfFamily_1_AutoML_1_202602...  0.933759  0.884405   \n",
       "5                      GBM_1_AutoML_1_20260201_163031  0.933149  0.882843   \n",
       "6                      GBM_5_AutoML_1_20260201_163031  0.932727  0.886228   \n",
       "7                      GBM_4_AutoML_1_20260201_163031  0.931311  0.885446   \n",
       "8         GBM_grid_1_AutoML_1_20260201_163031_model_5  0.931226  0.885707   \n",
       "9                      GBM_3_AutoML_1_20260201_163031  0.930746  0.884145   \n",
       "10                                  XGBoost_Optimized  0.930429  0.881281   \n",
       "11        GBM_grid_1_AutoML_1_20260201_163031_model_4  0.929151  0.883364   \n",
       "12        GBM_grid_1_AutoML_1_20260201_163031_model_2  0.927931  0.885186   \n",
       "13                     DRF_1_AutoML_1_20260201_163031  0.927341  0.876855   \n",
       "14  DeepLearning_grid_1_AutoML_1_20260201_163031_m...  0.925463  0.874251   \n",
       "\n",
       "    f1_score  \n",
       "0   0.911656  \n",
       "1   0.910512  \n",
       "2   0.908981  \n",
       "3   0.907453  \n",
       "4   0.909571  \n",
       "5   0.908833  \n",
       "6   0.909863  \n",
       "7   0.911075  \n",
       "8   0.910277  \n",
       "9   0.910264  \n",
       "10  0.905079  \n",
       "11  0.908582  \n",
       "12  0.910311  \n",
       "13  0.903292  \n",
       "14  0.902507  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** BEST MODEL FROM EACH SOURCE ***\n",
      "\n",
      "baseline:\n",
      "  Rank: #3\n",
      "  Model: GBM_baseline\n",
      "  AUC: 0.9353\n",
      "  Accuracy: 0.8834\n",
      "\n",
      "H2O_AutoML:\n",
      "  Rank: #1\n",
      "  Model: StackedEnsemble_AllModels_1_AutoML_1_20260201_163031\n",
      "  AUC: 0.9364\n",
      "  Accuracy: 0.8875\n",
      "\n",
      "XGBoost_Optuna:\n",
      "  Rank: #11\n",
      "  Model: XGBoost_Optimized\n",
      "  AUC: 0.9304\n",
      "  Accuracy: 0.8813\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>source</th>\n",
       "      <th>model_type</th>\n",
       "      <th>model_name</th>\n",
       "      <th>auc</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>logloss</th>\n",
       "      <th>rank_in_source</th>\n",
       "      <th>overall_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-02-01 16:54:35</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>StackedEnsemble</td>\n",
       "      <td>StackedEnsemble_AllModels_1_AutoML_1_20260201_...</td>\n",
       "      <td>0.936405</td>\n",
       "      <td>0.887529</td>\n",
       "      <td>0.911656</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.280824</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-02-01 16:54:35</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_2_AutoML_1_20260201_163031</td>\n",
       "      <td>0.935718</td>\n",
       "      <td>0.887529</td>\n",
       "      <td>0.910512</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.285363</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-02-01 16:30:26</td>\n",
       "      <td>baseline</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_baseline</td>\n",
       "      <td>0.935325</td>\n",
       "      <td>0.883364</td>\n",
       "      <td>0.908981</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.287731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-02-01 16:54:35</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_grid_1_AutoML_1_20260201_163031_model_1</td>\n",
       "      <td>0.934196</td>\n",
       "      <td>0.882583</td>\n",
       "      <td>0.907453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.292350</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-02-01 16:54:35</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>StackedEnsemble</td>\n",
       "      <td>StackedEnsemble_BestOfFamily_1_AutoML_1_202602...</td>\n",
       "      <td>0.933759</td>\n",
       "      <td>0.884405</td>\n",
       "      <td>0.909571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.286647</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2026-02-01 16:54:35</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_1_AutoML_1_20260201_163031</td>\n",
       "      <td>0.933149</td>\n",
       "      <td>0.882843</td>\n",
       "      <td>0.908833</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.292054</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2026-02-01 16:54:35</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_5_AutoML_1_20260201_163031</td>\n",
       "      <td>0.932727</td>\n",
       "      <td>0.886228</td>\n",
       "      <td>0.909863</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.291905</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2026-02-01 16:54:35</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_4_AutoML_1_20260201_163031</td>\n",
       "      <td>0.931311</td>\n",
       "      <td>0.885446</td>\n",
       "      <td>0.911075</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.298327</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2026-02-01 16:54:35</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_grid_1_AutoML_1_20260201_163031_model_5</td>\n",
       "      <td>0.931226</td>\n",
       "      <td>0.885707</td>\n",
       "      <td>0.910277</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.295183</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2026-02-01 16:54:35</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_3_AutoML_1_20260201_163031</td>\n",
       "      <td>0.930746</td>\n",
       "      <td>0.884145</td>\n",
       "      <td>0.910264</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.294040</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2026-02-01 16:54:35</td>\n",
       "      <td>XGBoost_Optuna</td>\n",
       "      <td>XGBoost_Optimized</td>\n",
       "      <td>XGBoost_Optimized</td>\n",
       "      <td>0.930429</td>\n",
       "      <td>0.881281</td>\n",
       "      <td>0.905079</td>\n",
       "      <td>0.872392</td>\n",
       "      <td>0.940311</td>\n",
       "      <td>0.301532</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2026-02-01 16:54:35</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_grid_1_AutoML_1_20260201_163031_model_4</td>\n",
       "      <td>0.929151</td>\n",
       "      <td>0.883364</td>\n",
       "      <td>0.908582</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.308798</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2026-02-01 16:54:35</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>GBM</td>\n",
       "      <td>GBM_grid_1_AutoML_1_20260201_163031_model_2</td>\n",
       "      <td>0.927931</td>\n",
       "      <td>0.885186</td>\n",
       "      <td>0.910311</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.305271</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2026-02-01 16:54:35</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>DRF</td>\n",
       "      <td>DRF_1_AutoML_1_20260201_163031</td>\n",
       "      <td>0.927341</td>\n",
       "      <td>0.876855</td>\n",
       "      <td>0.903292</td>\n",
       "      <td>0.988338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.337781</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2026-02-01 16:54:35</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>DeepLearning</td>\n",
       "      <td>DeepLearning_grid_1_AutoML_1_20260201_163031_m...</td>\n",
       "      <td>0.925463</td>\n",
       "      <td>0.874251</td>\n",
       "      <td>0.902507</td>\n",
       "      <td>0.985294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.322566</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2026-02-01 16:54:35</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>DeepLearning</td>\n",
       "      <td>DeepLearning_grid_3_AutoML_1_20260201_163031_m...</td>\n",
       "      <td>0.925402</td>\n",
       "      <td>0.876855</td>\n",
       "      <td>0.903200</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.353212</td>\n",
       "      <td>19.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2026-02-01 16:30:31</td>\n",
       "      <td>baseline</td>\n",
       "      <td>DRF</td>\n",
       "      <td>DRF_baseline</td>\n",
       "      <td>0.924562</td>\n",
       "      <td>0.868003</td>\n",
       "      <td>0.896970</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.349554</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2026-02-01 16:54:35</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>DeepLearning</td>\n",
       "      <td>DeepLearning_1_AutoML_1_20260201_163031</td>\n",
       "      <td>0.924214</td>\n",
       "      <td>0.876334</td>\n",
       "      <td>0.904187</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.329521</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2026-02-01 16:54:35</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>DeepLearning</td>\n",
       "      <td>DeepLearning_grid_2_AutoML_1_20260201_163031_m...</td>\n",
       "      <td>0.922261</td>\n",
       "      <td>0.876595</td>\n",
       "      <td>0.904049</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.323769</td>\n",
       "      <td>15.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2026-02-01 16:54:35</td>\n",
       "      <td>H2O_AutoML</td>\n",
       "      <td>DeepLearning</td>\n",
       "      <td>DeepLearning_grid_3_AutoML_1_20260201_163031_m...</td>\n",
       "      <td>0.920970</td>\n",
       "      <td>0.876074</td>\n",
       "      <td>0.903422</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.318696</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              timestamp          source         model_type  \\\n",
       "0   2026-02-01 16:54:35      H2O_AutoML    StackedEnsemble   \n",
       "1   2026-02-01 16:54:35      H2O_AutoML                GBM   \n",
       "2   2026-02-01 16:30:26        baseline                GBM   \n",
       "3   2026-02-01 16:54:35      H2O_AutoML                GBM   \n",
       "4   2026-02-01 16:54:35      H2O_AutoML    StackedEnsemble   \n",
       "5   2026-02-01 16:54:35      H2O_AutoML                GBM   \n",
       "6   2026-02-01 16:54:35      H2O_AutoML                GBM   \n",
       "7   2026-02-01 16:54:35      H2O_AutoML                GBM   \n",
       "8   2026-02-01 16:54:35      H2O_AutoML                GBM   \n",
       "9   2026-02-01 16:54:35      H2O_AutoML                GBM   \n",
       "10  2026-02-01 16:54:35  XGBoost_Optuna  XGBoost_Optimized   \n",
       "11  2026-02-01 16:54:35      H2O_AutoML                GBM   \n",
       "12  2026-02-01 16:54:35      H2O_AutoML                GBM   \n",
       "13  2026-02-01 16:54:35      H2O_AutoML                DRF   \n",
       "14  2026-02-01 16:54:35      H2O_AutoML       DeepLearning   \n",
       "15  2026-02-01 16:54:35      H2O_AutoML       DeepLearning   \n",
       "16  2026-02-01 16:30:31        baseline                DRF   \n",
       "17  2026-02-01 16:54:35      H2O_AutoML       DeepLearning   \n",
       "18  2026-02-01 16:54:35      H2O_AutoML       DeepLearning   \n",
       "19  2026-02-01 16:54:35      H2O_AutoML       DeepLearning   \n",
       "\n",
       "                                           model_name       auc  accuracy  \\\n",
       "0   StackedEnsemble_AllModels_1_AutoML_1_20260201_...  0.936405  0.887529   \n",
       "1                      GBM_2_AutoML_1_20260201_163031  0.935718  0.887529   \n",
       "2                                        GBM_baseline  0.935325  0.883364   \n",
       "3         GBM_grid_1_AutoML_1_20260201_163031_model_1  0.934196  0.882583   \n",
       "4   StackedEnsemble_BestOfFamily_1_AutoML_1_202602...  0.933759  0.884405   \n",
       "5                      GBM_1_AutoML_1_20260201_163031  0.933149  0.882843   \n",
       "6                      GBM_5_AutoML_1_20260201_163031  0.932727  0.886228   \n",
       "7                      GBM_4_AutoML_1_20260201_163031  0.931311  0.885446   \n",
       "8         GBM_grid_1_AutoML_1_20260201_163031_model_5  0.931226  0.885707   \n",
       "9                      GBM_3_AutoML_1_20260201_163031  0.930746  0.884145   \n",
       "10                                  XGBoost_Optimized  0.930429  0.881281   \n",
       "11        GBM_grid_1_AutoML_1_20260201_163031_model_4  0.929151  0.883364   \n",
       "12        GBM_grid_1_AutoML_1_20260201_163031_model_2  0.927931  0.885186   \n",
       "13                     DRF_1_AutoML_1_20260201_163031  0.927341  0.876855   \n",
       "14  DeepLearning_grid_1_AutoML_1_20260201_163031_m...  0.925463  0.874251   \n",
       "15  DeepLearning_grid_3_AutoML_1_20260201_163031_m...  0.925402  0.876855   \n",
       "16                                       DRF_baseline  0.924562  0.868003   \n",
       "17            DeepLearning_1_AutoML_1_20260201_163031  0.924214  0.876334   \n",
       "18  DeepLearning_grid_2_AutoML_1_20260201_163031_m...  0.922261  0.876595   \n",
       "19  DeepLearning_grid_3_AutoML_1_20260201_163031_m...  0.920970  0.876074   \n",
       "\n",
       "    f1_score  precision    recall   logloss  rank_in_source  overall_rank  \n",
       "0   0.911656   1.000000  1.000000  0.280824             1.0             1  \n",
       "1   0.910512   1.000000  1.000000  0.285363             5.0             2  \n",
       "2   0.908981   1.000000  1.000000  0.287731             NaN             3  \n",
       "3   0.907453   1.000000  1.000000  0.292350             6.0             4  \n",
       "4   0.909571   1.000000  1.000000  0.286647             2.0             5  \n",
       "5   0.908833   1.000000  1.000000  0.292054             4.0             6  \n",
       "6   0.909863   1.000000  1.000000  0.291905             3.0             7  \n",
       "7   0.911075   1.000000  1.000000  0.298327             7.0             8  \n",
       "8   0.910277   1.000000  1.000000  0.295183             8.0             9  \n",
       "9   0.910264   1.000000  1.000000  0.294040            10.0            10  \n",
       "10  0.905079   0.872392  0.940311  0.301532             1.0            11  \n",
       "11  0.908582   1.000000  1.000000  0.308798             9.0            12  \n",
       "12  0.910311   1.000000  1.000000  0.305271            11.0            13  \n",
       "13  0.903292   0.988338  1.000000  0.337781            12.0            14  \n",
       "14  0.902507   0.985294  1.000000  0.322566            13.0            15  \n",
       "15  0.903200   0.969697  1.000000  0.353212            19.0            16  \n",
       "16  0.896970   1.000000  1.000000  0.349554             NaN            17  \n",
       "17  0.904187   0.988095  1.000000  0.329521            17.0            18  \n",
       "18  0.904049   0.972973  1.000000  0.323769            15.0            19  \n",
       "19  0.903422   0.983333  1.000000  0.318696            16.0            20  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate ALL H2O AutoML models + XGBoost\n",
    "print('='*70)\n",
    "print('COMPREHENSIVE MODEL EVALUATION')\n",
    "print('='*70)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Evaluate H2O AutoML models\n",
    "if RUN_AUTOML and aml is not None:\n",
    "    print()\n",
    "    print('*** Evaluating H2O AutoML Models ***')\n",
    "    leaderboard = aml.leaderboard.as_data_frame()\n",
    "    print(f'Models from H2O AutoML: {len(leaderboard)}')\n",
    "\n",
    "    for idx, model_id in enumerate(leaderboard['model_id'], 1):\n",
    "        if idx <= 5 or idx > len(leaderboard) - 5:\n",
    "            print(f'  Model {idx}/{len(leaderboard)}: {model_id}')\n",
    "        elif idx == 6:\n",
    "            print(f'  ... processing {len(leaderboard) - 10} models ...')\n",
    "\n",
    "        model = h2o.get_model(model_id)\n",
    "        perf = model.model_performance(valid=True)\n",
    "\n",
    "        auc = perf.auc()\n",
    "        acc = perf.accuracy()[0][1] if perf.accuracy() else None\n",
    "        f1 = perf.F1()[0][1] if perf.F1() else None\n",
    "        precision = perf.precision()[0][1] if perf.precision() else None\n",
    "        recall = perf.recall()[0][1] if perf.recall() else None\n",
    "        logloss = perf.logloss()\n",
    "\n",
    "        model_type = model_id.split('_')[0]\n",
    "\n",
    "        all_results.append({\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'source': 'H2O_AutoML',\n",
    "            'model_type': model_type,\n",
    "            'model_name': model_id,\n",
    "            'auc': auc,\n",
    "            'accuracy': acc,\n",
    "            'f1_score': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'logloss': logloss,\n",
    "            'rank_in_source': idx\n",
    "        })\n",
    "else:\n",
    "    print()\n",
    "    print('Skipping H2O AutoML evaluation (not run)')\n",
    "\n",
    "# Add XGBoost result\n",
    "if RUN_XGBOOST and xgb_metrics is not None:\n",
    "    print()\n",
    "    print('*** Adding XGBoost Model ***')\n",
    "    xgb_metrics['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    xgb_metrics['source'] = 'XGBoost_Optuna'\n",
    "    xgb_metrics['model_name'] = 'XGBoost_Optimized'\n",
    "    xgb_metrics['rank_in_source'] = 1\n",
    "    all_results.append(xgb_metrics)\n",
    "    print(f\"  XGBoost AUC: {xgb_metrics['auc']:.4f}\")\n",
    "else:\n",
    "    print()\n",
    "    print('Skipping XGBoost evaluation (not run)')\n",
    "\n",
    "# Combine with baseline results\n",
    "all_results_combined = pd.concat([baseline_results, pd.DataFrame(all_results)], ignore_index=True)\n",
    "all_results_combined = all_results_combined.sort_values('auc', ascending=False).reset_index(drop=True)\n",
    "all_results_combined['overall_rank'] = range(1, len(all_results_combined) + 1)\n",
    "\n",
    "# Save results\n",
    "results_file = f'results/model_logs_complete_{timestamp_str}.csv'\n",
    "all_results_combined.to_csv(results_file, index=False)\n",
    "\n",
    "# Save just H2O AutoML\n",
    "h2o_results = all_results_combined[all_results_combined['source'] == 'H2O_AutoML']\n",
    "h2o_results.to_csv(f'results/model_logs_h2o_automl_{timestamp_str}.csv', index=False)\n",
    "\n",
    "# Save just XGBoost\n",
    "xgb_results = all_results_combined[all_results_combined['source'] == 'XGBoost_Optuna']\n",
    "xgb_results.to_csv(f'results/model_logs_xgboost_{timestamp_str}.csv', index=False)\n",
    "\n",
    "print()\n",
    "print('='*70)\n",
    "print('RESULTS SUMMARY')\n",
    "print('='*70)\n",
    "print(f'Total models evaluated: {len(all_results_combined)}')\n",
    "print(f'  - Baseline: {len(baseline_results)}')\n",
    "print(f'  - H2O AutoML: {len(h2o_results)}')\n",
    "print(f'  - XGBoost: {len(xgb_results)}')\n",
    "print()\n",
    "print('Files saved:')\n",
    "print(f'  - {results_file} (ALL models)')\n",
    "print(f'  - results/model_logs_h2o_automl_{timestamp_str}.csv')\n",
    "print(f'  - results/model_logs_xgboost_{timestamp_str}.csv')\n",
    "\n",
    "print()\n",
    "print('*** TOP 15 MODELS OVERALL (by AUC) ***')\n",
    "display(all_results_combined[['overall_rank', 'source', 'model_type', 'model_name', 'auc', 'accuracy', 'f1_score']].head(15))\n",
    "\n",
    "print()\n",
    "print('*** BEST MODEL FROM EACH SOURCE ***')\n",
    "for source in ['baseline', 'H2O_AutoML', 'XGBoost_Optuna']:\n",
    "    source_results = all_results_combined[all_results_combined['source'] == source]\n",
    "    if len(source_results) > 0:\n",
    "        best = source_results.iloc[0]\n",
    "        print()\n",
    "        print(f\"{source}:\")\n",
    "        print(f\"  Rank: #{best['overall_rank']}\")\n",
    "        print(f\"  Model: {best['model_name']}\")\n",
    "        print(f\"  AUC: {best['auc']:.4f}\")\n",
    "        print(f\"  Accuracy: {best['accuracy']:.4f}\")\n",
    "\n",
    "all_results_combined.head(20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
